{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_shot = 0\n",
    "explain = False\n",
    "save_dir = \".\"\n",
    "dataset = \"copa\"\n",
    "\n",
    "# Disclaimer: This GPT-3 code is to demonstrate the logic of running and obtaining results from GPT-3.\n",
    "# This notebook and related code were written *after* the result files .bin were obtained.\n",
    "# We are unable to test this code as we ran out of OpenAI free credits :(\n",
    "# Nevertheless, if one can run this notebook without errors, one should get very close results to the report,\n",
    "# modulo the randomness in GPT-3 generation results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import time\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from run_gpt3 import prepare_copa, prepare_ecare, get_prompts_with_labels, save_results, get_gpt3_prediction\n",
    "import openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert explain == False or dataset == \"ecare\", \"COPA does not have explanations\"\n",
    "\n",
    "openai.api_key_path = \"/path/to/text/file/with/openai/api/key\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"copa\":\n",
    "    train_set, dev_set = prepare_copa()\n",
    "elif dataset == \"ecare\":\n",
    "    train_set, dev_set = prepare_ecare()\n",
    "prompts = get_prompts_with_labels(train_set, dev_set, k_shot, explain)\n",
    "gpt_preds = []\n",
    "curr_prompt_idx = 0\n",
    "prompts_submitted = {k: False for k in range(prompts.num_rows)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell can be interruppted and resumed arbitrary\n",
    "\n",
    "print(f\"Started running from example #{curr_prompt_idx}\")\n",
    "while True:\n",
    "    try:\n",
    "        if curr_prompt_idx == prompts.num_rows:\n",
    "            print(\"Finished.\")\n",
    "            break\n",
    "        if not prompts_submitted[curr_prompt_idx]:\n",
    "            prompt = prompts[curr_prompt_idx][\"prompt\"]\n",
    "            pred = get_gpt3_prediction(prompt)\n",
    "            gpt_preds.append(pred)\n",
    "            prompts_submitted[curr_prompt_idx] = True\n",
    "        curr_prompt_idx += 1\n",
    "    except openai.error.RateLimitError:\n",
    "        print(f\"Sleeping at example #{curr_prompt_idx}.\")\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Interrupted at example #{curr_prompt_idx}. Pausing.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gpt_preds, prompts, k_shot=k_shot, dataset=dataset, explain=explain, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct  7 2022, 20:14:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42cbe137d1eb0b05077e25ded00aae12b48391c903ae2922613505c4ac843849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
