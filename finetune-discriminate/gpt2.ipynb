{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gusFYo42RKLq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yTLfkN-FRBJS"
      },
      "outputs": [],
      "source": [
        "dataset = \"ecare\"\n",
        "model_checkpoint = \"gpt2\"\n",
        "global_seed = 139\n",
        "epochs = 3\n",
        "lr = 5e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These notebooks are best run on Colab. Because of this, I made each notebook as independent and self-contained as possible, even if they have overlapping code. These notebooks assume a GPU is available.\n",
        "\n",
        "To replicate:\n",
        "\n",
        "* e-CARE:\n",
        "```\n",
        "global_seed = 139\n",
        "epochs = 3\n",
        "lr = 5e-5\n",
        "```\n",
        "\n",
        "* COPA:\n",
        "```\n",
        "global_seed = 139\n",
        "epochs = 10\n",
        "lr = 5e-5\n",
        "```"
      ],
      "metadata": {
        "id": "ItzmhDCRajKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and setup"
      ],
      "metadata": {
        "id": "gusFYo42RKLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq transformers[sentencepiece] datasets evaluate\n",
        "!wget https://github.com/Waste-Wood/e-CARE/files/8242580/e-CARE.zip"
      ],
      "metadata": {
        "id": "ofVcC0cdRLLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa7e278-f8d0-4b80-b622-dea1c9de8793"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-21 02:02:36--  https://github.com/Waste-Wood/e-CARE/files/8242580/e-CARE.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-repository-file-5c1aeb/465962344/8242580?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221221T020236Z&X-Amz-Expires=300&X-Amz-Signature=7d9945f7f88b858ac0f131eea062a77cdf87b4be0442e00273d6495b9cf02fb2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=465962344&response-content-disposition=attachment%3Bfilename%3De-CARE.zip&response-content-type=application%2Fzip [following]\n",
            "--2022-12-21 02:02:37--  https://objects.githubusercontent.com/github-production-repository-file-5c1aeb/465962344/8242580?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221221T020236Z&X-Amz-Expires=300&X-Amz-Signature=7d9945f7f88b858ac0f131eea062a77cdf87b4be0442e00273d6495b9cf02fb2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=465962344&response-content-disposition=attachment%3Bfilename%3De-CARE.zip&response-content-type=application%2Fzip\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3304152 (3.2M) [application/zip]\n",
            "Saving to: ‘e-CARE.zip.2’\n",
            "\n",
            "e-CARE.zip.2        100%[===================>]   3.15M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-21 02:02:37 (252 MB/s) - ‘e-CARE.zip.2’ saved [3304152/3304152]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import evaluate\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import transformers\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "import random\n",
        "import zipfile\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union\n",
        "\n",
        "# Some of the code is inspired by\n",
        "# https://github.com/Waste-Wood/e-CARE/blob/main/code/gpt2_discriminate.py"
      ],
      "metadata": {
        "id": "tOTpX2XQRMik"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load('super_glue', 'copa')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
        "if global_seed is not None:\n",
        "    random.seed(global_seed)\n",
        "    np.random.seed(global_seed)\n",
        "    torch.manual_seed(global_seed)\n",
        "    torch.cuda.manual_seed(global_seed)"
      ],
      "metadata": {
        "id": "E3phrYgWROB3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function definitions"
      ],
      "metadata": {
        "id": "Q3Vrq59nRndU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Discriminate(nn.Module):\n",
        "    def __init__(self, model_checkpoint):\n",
        "        super(GPT2Discriminate, self).__init__()\n",
        "        self.model = GPT2Model.from_pretrained(model_checkpoint)\n",
        "        self.linear = nn.Linear(self.model.config.hidden_size, 1)\n",
        "        self.loss_function = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "    def forward(self, *, input_ids=None, attention_mask=None, pos=None, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_state = outputs.last_hidden_state\n",
        "        pos = pos.squeeze().unsqueeze(0)\n",
        "        hidden_state = hidden_state[range(hidden_state.shape[0]), pos, :].squeeze(0)\n",
        "        logits = self.linear(hidden_state).squeeze(-1)\n",
        "        loss = self.loss_function(logits, labels)\n",
        "        results = transformers.modeling_outputs.SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "        )\n",
        "        return results\n",
        "\n",
        "\n",
        "def convert_choice(choice):\n",
        "    # de-capitalizes the first character\n",
        "    return choice[0].lower() + choice[1:]\n",
        "\n",
        "\n",
        "def convert_premise(premise):\n",
        "    # removes the full stop\n",
        "    return premise.strip()[:-1]\n",
        "\n",
        "\n",
        "def concat_premise_choice(datapoint, return_text_only=False):\n",
        "    premise = datapoint[\"premise\"]\n",
        "    choice1 = datapoint[\"choice1\"]\n",
        "    choice2 = datapoint[\"choice2\"]\n",
        "    question = datapoint[\"question\"]\n",
        "    label = datapoint[\"label\"]\n",
        "    if question == \"cause\":\n",
        "        causal_relation_1 = convert_premise(premise) + \" because \" + convert_choice(choice1)\n",
        "        causal_relation_2 = convert_premise(premise) + \" because \" + convert_choice(choice2)\n",
        "    elif question == \"effect\":\n",
        "        causal_relation_1 = convert_premise(choice1) + \" because \" + convert_choice(premise)\n",
        "        causal_relation_2 = convert_premise(choice2) + \" because \" + convert_choice(premise)\n",
        "    if return_text_only:\n",
        "        return causal_relation_1, causal_relation_2\n",
        "    return [\n",
        "        {\"relation\": causal_relation_1, \"label\": 1-label},\n",
        "        {\"relation\": causal_relation_2, \"label\": label}\n",
        "    ]\n",
        "\n",
        "\n",
        "def tokenization(tokenizer, data):\n",
        "    \"\"\"Tokenizes and manually pads each causal relation\n",
        "    data is a list of dicts \n",
        "    \"\"\"\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    pos = []\n",
        "\n",
        "    for example in data:\n",
        "        causal_relation_1, causal_relation_2 = concat_premise_choice(example, return_text_only=True)\n",
        "        inputs.extend([causal_relation_1, causal_relation_2])\n",
        "        labels += [0, 1] if example['label'] == 1 else [1, 0]\n",
        "    outputs = tokenizer(inputs, return_length=True)\n",
        "    input_ids = outputs['input_ids']\n",
        "    attention_mask = outputs['attention_mask']\n",
        "    length = outputs['length']\n",
        "    max_length = max(length)\n",
        "    for i in range(len(input_ids)):\n",
        "        gap = max_length - len(input_ids[i]) + 1\n",
        "        pos.append(len(input_ids[i]))\n",
        "        input_ids[i] += [50256 for _ in range(gap)]\n",
        "        attention_mask[i] += [1] + [0 for _ in range(gap-1)]\n",
        "    return {\"input_ids\": torch.LongTensor(input_ids), \"attention_mask\": torch.LongTensor(attention_mask), \"pos\": torch.LongTensor(pos), \"label\": torch.FloatTensor(labels)}\n",
        "\n",
        "\n",
        "def compute_metrics(eval_predictions):\n",
        "    \"\"\"For use in hf trainer\n",
        "    eval_predictions is a namedtuple of numpy arrays\n",
        "    containing results over the whole dev set\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_predictions\n",
        "\n",
        "    # output vector of model is 1d\n",
        "    a1 = torch.FloatTensor(predictions[::2]).unsqueeze(1)\n",
        "    a2 = torch.FloatTensor(predictions[1::2]).unsqueeze(1)\n",
        "    a = torch.cat((a1, a2), dim=1)\n",
        "    predict_labels = torch.argmax(a, 1).tolist()\n",
        "\n",
        "    t_a1 = torch.FloatTensor(labels[::2]).unsqueeze(1)\n",
        "    t_a2 = torch.FloatTensor(labels[1::2]).unsqueeze(1)\n",
        "    t_a = torch.cat((t_a1, t_a2), dim=1)\n",
        "    true_labels = torch.argmax(t_a, 1).tolist()\n",
        "\n",
        "    accuracy = metric.compute(predictions=predict_labels, references=true_labels)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def process_choice(choice):\n",
        "    \"\"\"Used in evaluate_model\n",
        "    choice is a dict of lists (not tensors for some reason)\n",
        "    \"\"\"\n",
        "    input_ids = torch.LongTensor(choice[\"input_ids\"]).cuda().unsqueeze(0)\n",
        "    attention_mask = torch.LongTensor(choice[\"attention_mask\"]).cuda().unsqueeze(0)\n",
        "    pos = torch.LongTensor([choice[\"pos\"]]).cuda().unsqueeze(0)\n",
        "    labels = torch.FloatTensor([choice[\"label\"]]).cuda().squeeze()\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pos\": pos, \"labels\": labels}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, tokenized_data):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    for i in range(0, tokenized_data.num_rows, 2):\n",
        "        choice1 = tokenized_data[i]\n",
        "        choice2 = tokenized_data[i+1]\n",
        "        pred1 = model(**process_choice(choice1))\n",
        "        pred2 = model(**process_choice(choice2))\n",
        "        pred1 = pred1.logits.item()\n",
        "        pred2 = pred2.logits.item()\n",
        "        y_pred.append(torch.argmax(torch.FloatTensor([pred1, pred2])).item())\n",
        "        y_true.append(choice2[\"label\"])\n",
        "    return classification_report(y_true, y_pred), accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "tT4tYKP3Rp8f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "ZTGby_ZNZmB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == \"copa\":\n",
        "    copa = datasets.load_dataset(\"super_glue\", \"copa\")\n",
        "    # convert to hf Dataset object\n",
        "    tokenized_train = datasets.Dataset.from_dict(\n",
        "        tokenization(tokenizer, copa[\"train\"])\n",
        "    )\n",
        "    tokenized_dev = datasets.Dataset.from_dict(\n",
        "        tokenization(tokenizer, copa[\"validation\"])\n",
        "    )\n",
        "\n",
        "elif dataset == \"ecare\":\n",
        "    with zipfile.ZipFile(\"e-CARE.zip\") as z:\n",
        "        with z.open(\"dataset/train_full.jsonl\") as f:\n",
        "            train_df = pd.read_json(f, lines=True)\n",
        "        with z.open(\"dataset/dev_full.jsonl\") as f:\n",
        "            dev_df = pd.read_json(f, lines=True)\n",
        "\n",
        "    # rename columns same as copa\n",
        "    rel2fields = {\"ask-for\": \"question\", \"hypothesis1\": \"choice1\", \"hypothesis2\": \"choice2\", \"index\": \"idx\"}\n",
        "    train_df.rename(rel2fields, axis=1, inplace=True)\n",
        "    dev_df.rename(rel2fields, axis=1, inplace=True)\n",
        "\n",
        "    # convert to hf Dataset object\n",
        "    tokenized_train = datasets.Dataset.from_dict(\n",
        "        tokenization(tokenizer, train_df.to_dict(\"records\"))\n",
        "    )\n",
        "    tokenized_dev = datasets.Dataset.from_dict(\n",
        "        tokenization(tokenizer, dev_df.to_dict(\"records\"))\n",
        "    )"
      ],
      "metadata": {
        "id": "CtDp2g9pSaIR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Discriminate(model_checkpoint)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=1e-2,\n",
        "    # fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_dev,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "WtOLYmwtXmwy",
        "outputId": "6595fd6d-59c4-486f-a378-9a3412593a8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 29856\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2799\n",
            "  Number of trainable parameters = 124440577\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2799/2799 21:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.704800</td>\n",
              "      <td>0.694156</td>\n",
              "      <td>0.615928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.677500</td>\n",
              "      <td>0.664518</td>\n",
              "      <td>0.657870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.646200</td>\n",
              "      <td>0.650831</td>\n",
              "      <td>0.669651</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4244\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-933\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4244\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-1866\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4244\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-2799\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-2799 (score: 0.6696512723845429).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2799, training_loss=0.6729283579846458, metrics={'train_runtime': 1319.2056, 'train_samples_per_second': 67.895, 'train_steps_per_second': 2.122, 'total_flos': 0.0, 'train_loss': 0.6729283579846458, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classif_report, acc = evaluate_model(model, tokenized_dev)\n",
        "print(classif_report)\n",
        "print(\"Accuracy: {}\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gjit360XvTf",
        "outputId": "6688e86b-4ec3-463c-c912-e3efad2d68ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.66      0.67      1061\n",
            "         1.0       0.67      0.68      0.67      1061\n",
            "\n",
            "    accuracy                           0.67      2122\n",
            "   macro avg       0.67      0.67      0.67      2122\n",
            "weighted avg       0.67      0.67      0.67      2122\n",
            "\n",
            "Accuracy: 0.6696512723845429\n"
          ]
        }
      ]
    }
  ]
}